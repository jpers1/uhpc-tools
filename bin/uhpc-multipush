#!/usr/bin/env bash
#
# uhpc-multipush: Stream a local file into a specified path on multiple Slurm-allocated nodes.
#
# Part of uhpc-tools ("Unusual HPC Tools").
#
# Usage:
#   uhpc-multipush <LOCAL_FILE> <REMOTE_FILE> <NODE_LIST>
#
# Where:
#   LOCAL_FILE   = Path to the local file you want to copy, e.g. /dev/shm/data_local.dat
#   REMOTE_FILE  = Desired path/name on the remote nodes, e.g. /dev/shm/data_remote.dat
#   NODE_LIST    = Comma-separated node names, e.g. wn208,wn209
#
# This script will:
#   1) Check if the user has a Slurm allocation on each requested node.
#   2) If yes, it streams LOCAL_FILE over Slurm's I/O channel directly
#      into REMOTE_FILE on that node.
#   3) Avoids any shared filesystem usage, thus bypassing storage quota limits.
#
# Requirements:
#   - Slurm commands: srun, squeue, scontrol
#   - A valid Slurm allocation that includes each target node
#
# Example:
#   salloc --nodes=2 --nodelist=wn208,wn209 --time=1:00:00
#   uhpc-multipush /dev/shm/mydata_local.dat /dev/shm/mydata_remote.dat wn208,wn209
#
# The file /dev/shm/mydata_local.dat on your login node
# will appear as /dev/shm/mydata_remote.dat on wn208 and wn209.
#

set -o errexit  # Exit immediately if a command exits with a non-zero status
set -o nounset  # Treat unset variables as an error
# set -o xtrace  # (Optional) Debug: echo all commands

# ------------------------------------------------------------------------------
# FUNCTIONS
# ------------------------------------------------------------------------------

usage() {
  echo "Usage: $0 <LOCAL_FILE> <REMOTE_FILE> <NODE_LIST>"
  echo ""
  echo "  LOCAL_FILE   : Path to the file you want to copy (e.g. /dev/shm/data_local.dat)."
  echo "  REMOTE_FILE  : Path/name on the remote nodes (e.g. /dev/shm/data_remote.dat)."
  echo "  NODE_LIST    : Comma-separated node names (e.g. wn208,wn209)."
  echo ""
  echo "Example:"
  echo "  salloc --nodes=2 --nodelist=wn208,wn209 --time=1:00:00"
  echo "  $0 /dev/shm/mydata_local.dat /dev/shm/mydata_remote.dat wn208,wn209"
  exit 1
}

# Expand bracketed node ranges into one line per node
# e.g., "wn[208-209]" => "wn208" and "wn209"
expand_bracketed_nodes() {
  local bracket_expr="$1"
  scontrol show hostname "$bracket_expr" 2>/dev/null || true
}

# Check if the current user has an active Slurm allocation on the given node.
# Return 0 if yes, 1 if not.
user_has_allocation_on_node() {
  local node="$1"

  # Gather all nodes allocated to the user.
  # For each job, the node list might include bracket expressions: wn[208-209]
  # We'll expand them into actual node names, then search for a match.
  local allocated_list
  allocated_list="$(squeue -u "$USER" --noheader -o '%N' 2>/dev/null || true)"

  # If there's no output, user has no allocations
  [[ -z "$allocated_list" ]] && return 1

  # Expand bracket expressions line by line
  local final_nodes=()
  while read -r node_expr; do
    while read -r expanded; do
      final_nodes+=("$expanded")
    done < <(expand_bracketed_nodes "$node_expr")
  done <<< "$allocated_list"

  # Check if $node is among final_nodes
  for n in "${final_nodes[@]}"; do
    if [[ "$n" == "$node" ]]; then
      return 0
    fi
  done

  return 1
}

# ------------------------------------------------------------------------------
# MAIN
# ------------------------------------------------------------------------------

# Verify arguments
if [[ $# -ne 3 ]]; then
  usage
fi

LOCAL_FILE="$1"
REMOTE_FILE="$2"
NODE_LIST="$3"

# Check that LOCAL_FILE actually exists
if [[ ! -f "$LOCAL_FILE" ]]; then
  echo "ERROR: Local file '$LOCAL_FILE' does not exist or is not a regular file."
  exit 1
fi

# Split comma-separated NODE_LIST into array
IFS=',' read -ra NODES <<< "$NODE_LIST"

# For each node, confirm user has an allocation and then push the file
for node in "${NODES[@]}"; do
  if user_has_allocation_on_node "$node"; then
    echo "Copying '$LOCAL_FILE' to node '$node:$REMOTE_FILE' ..."
    # Stream the file into the nodeâ€™s REMOTE_FILE
    cat "$LOCAL_FILE" | srun --nodes=1 --ntasks=1 --nodelist="$node" \
        bash -c "cat > \"$REMOTE_FILE\""
  else
    echo "ERROR: No active Slurm allocation found on node '$node'."
    echo "       Allocate first, for example:"
    echo "         salloc --nodes=1 --nodelist=$node --time=1:00:00"
    exit 1
  fi
done

echo "Done. '$LOCAL_FILE' streamed to '$REMOTE_FILE' on nodes: ${NODES[*]}"
exit 0
